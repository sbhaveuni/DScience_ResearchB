{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687e2ce3",
   "metadata": {},
   "source": [
    "## Final notebook RAG based chatbot 1 and 2 2nd includes new foi text too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0add8f6",
   "metadata": {},
   "source": [
    "## Gradio Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8a131",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977c565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\miniconda3\\envs\\finetune\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# rag_chatbot.py\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os, re, json, math, textwrap, requests\n",
    "from collections import Counter, defaultdict\n",
    "from keybert import KeyBERT\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchAny, Range\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Any, Dict, List, Optional\n",
    "# from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchAny, Range\n",
    "# ================== Config ==================\n",
    "\n",
    "\n",
    "COLLECTION    = os.getenv(\"QDRANT_COLLECTION\", \"test_rag\")#test_rag,maude_adverse_events\n",
    "EMBED_MODEL   = os.getenv(\"EMBED_MODEL\", \"all-MiniLM-L6-v2\")\n",
    "OLLAMA_URL    = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL  = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "\n",
    "TOP_K         = 30       # search limit\n",
    "MIN_HITS      = 5        # relax threshold\n",
    "MAX_HITS      = 50       # final target\n",
    "\n",
    "# ================== Clients ==================\n",
    "# client  = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "QDRANT_URL = \"https://9e2bd0a9-92b2-41c6-a0e7-01ae1eca199d.us-west-2-0.aws.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.oZGy02grPTOsR8Up1bN_rZfQCJHB6i0a2N3LJwN0H_E\"\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "encoder = SentenceTransformer(EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8846c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Define your label set in the exact form the model outputs\n",
    "label_list = [\"Death\", \"Injury\", \"Device Malfunction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023fe21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# %% [Load Quantized Base Model + Tokenizer]\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "compute_dtype = torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1b224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, \"trained-model\")\n",
    "model.eval()\n",
    "model.config.use_cache = False  # <- THIS IS CRUCIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Models =====\n",
    "bert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model=bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_TERMS = {\n",
    "    \"Device Malfunction\": [\n",
    "        # General device failure\n",
    "        \"device malfunction\", \"equipment malfunction\", \"malfunction during use\",\n",
    "        \"device not working\", \"device failed\", \"failure during operation\",\n",
    "        \"failure to function\", \"malfunction on deployment\", \"malfunction after use\",\n",
    "\n",
    "        # Deployment / operational issues\n",
    "        \"deployment failure\", \"incomplete deployment\", \"unable to deploy\",\n",
    "        \"failed to deploy\", \"partial deployment\", \"deployment interrupted\",\n",
    "        \"failed to lock\", \"locking issue\", \"activation failure\",\n",
    "        \"device misfire\", \"delivery issue\", \"misalignment of device\",\n",
    "\n",
    "        # Structural / mechanical failures\n",
    "        \"device damage\", \"component breakage\", \"handle breakage\",\n",
    "        \"material integrity issue\", \"broken tip\", \"torn sleeve\",\n",
    "        \"kinked sheath\", \"catheter kink\", \"component dislodged\",\n",
    "        \"sealant exposed\", \"loose seal\", \"sealant leakage\", \"sealant failure\",\n",
    "\n",
    "        # Blockages and leaks\n",
    "        \"balloon leak\", \"balloon burst\", \"balloon deflation\",\n",
    "        \"unable to inflate\", \"leak in system\", \"fluid leakage\",\n",
    "\n",
    "        # Obstruction / entrapment\n",
    "        \"device stuck\", \"entrapment issue\", \"obstructed device\",\n",
    "        \"retained device\", \"unable to remove device\",\n",
    "\n",
    "        # Electronics / software\n",
    "        \"system error\", \"software error\", \"firmware bug\",\n",
    "        \"power failure\", \"battery failure\", \"low battery alert\",\n",
    "        \"display malfunction\", \"false alarm\", \"incorrect reading\",\n",
    "\n",
    "        # Extra from your extended set\n",
    "        \"cuff miss\", \"quality issue\", \"reported difficulty\", \"device returned\", \n",
    "        \"device behavior\", \"device performance\", \"reported malfunction\", \n",
    "        \"returned for analysis\", \"proglide failure\", \"mechanical issue\",\n",
    "        \"clicking sound\", \"double click\", \"clicks on activation\",\n",
    "        \"device click\", \"unexpected click\", \"plunger click\",\n",
    "        \"audible click\", \"strange sound\", \"device noise\", \n",
    "        \"mechanical feedback\", \"anchor failed\", \"footplate broken\", \n",
    "        \"collagen plug issue\", \"plug misplacement\", \"clip misfire\", \n",
    "        \"clip dislodged\", \"anchor not seated\", \"prostar failure\",\n",
    "        \"device mispositioned\", \"poor seal\", \"incomplete closure\", \n",
    "        \"deployment drift\", \"suture break\", \"clip snapped\", \"cord rupture\", \n",
    "        \"vcd jammed\", \"advancer stuck\", \"release failure\", \n",
    "        \"footing not deployed\", \"clip ejected\", \"seal not achieved\", \"device knot\",\n",
    "        \"deployment issue\", \"catheter jammed\", \"stuck device\"\n",
    "    ],\n",
    "\n",
    "    \"Injury\": [\n",
    "        # Bleeding / hematoma\n",
    "        \"bleeding\", \"excessive bleeding\", \"control of bleeding\",\n",
    "        \"inability to stop bleeding\", \"hemostasis failure\", \"manual compression\",\n",
    "        \"hematoma\", \"hematoma formation\", \"site hematoma\",\n",
    "\n",
    "        # Pain & discomfort\n",
    "        \"pain\", \"site pain\", \"localized pain\", \"post-procedure pain\",\n",
    "        \"site discomfort\", \"discomfort\", \"local irritation\",\n",
    "\n",
    "        # Infections & inflammation\n",
    "        \"infection\", \"site infection\", \"wound infection\",\n",
    "        \"inflammatory reaction\", \"swelling\", \"redness\",\n",
    "        \"bruising\", \"seroma\", \"delayed healing\", \"wound complication\",\n",
    "\n",
    "        # Vascular / tissue injuries\n",
    "        \"vessel injury\", \"vascular injury\", \"arterial injury\",\n",
    "        \"tissue damage\", \"tissue necrosis\", \"necrosis\",\n",
    "        \"pseudoaneurysm\", \"extravasation\", \"vascular spasm\",\n",
    "        \"ischemia\", \"limb ischemia\", \"nerve injury\", \"limb numbness\", \"numbness\",\n",
    "\n",
    "        # Hypersensitivity / allergy\n",
    "        \"hypersensitivity reaction\", \"allergic reaction\", \"skin reaction\",\n",
    "        \"contact dermatitis\", \"rash\", \"burns\",\n",
    "\n",
    "        # Extra from your extended set\n",
    "        \"hematosis\", \"occlusion\", \"retroperitoneal bleed\", \"av fistula\", \n",
    "        \"femoral pseudoaneurysm\", \"vessel perforation\", \"artery dissection\", \n",
    "        \"groin pain\", \"groin hematoma\", \"thrombus formation\", \n",
    "        \"deep vein thrombosis\", \"vascular laceration\", \"prolonged bleeding\", \n",
    "        \"puncture site complication\", \"arterial embolism\",\"skin reaction\"\n",
    "    ],\n",
    "\n",
    "    \"Death\": [\n",
    "        # Direct death reports\n",
    "        \"patient died\", \"patient death\", \"death reported\",\n",
    "        \"death occurred\", \"fatal event\", \"fatal outcome\",\n",
    "        \"loss of life\", \"mortality\", \"fatality\", \"lethal event\",\n",
    "        \"procedure-related death\", \"unexpected death\", \"sudden death\",\n",
    "        \"death following procedure\", \"fatal complication\",\n",
    "\n",
    "        # Common MAUDE phrasings\n",
    "        \"patient demise\", \"passed away\", \"found deceased\",\n",
    "        \"collapsed and died\", \"expired\", \"death confirmed\",\n",
    "        \"pronounced dead\", \"declared dead\", \"cause of death\",\n",
    "        \"autopsy revealed\", \"post mortem\", \"post-mortem\",\n",
    "        \"death certificate\", \"no signs of life\", \"end of life\",\n",
    "\n",
    "        # Indirect death phrasing\n",
    "        \"died suddenly\", \"died during procedure\", \"died after procedure\",\n",
    "\n",
    "        # Extra from your extended set\n",
    "        \"cardiac arrest during procedure\", \"exsanguination\", \"massive bleed\",\n",
    "        \"vascular collapse\", \"procedure fatality\", \"died from complication\",\n",
    "        \"bleed-out\", \"femoral rupture\", \"death due to AV fistula\", \n",
    "        \"fatal stroke\", \"multi-organ failure\", \"cardiovascular collapse\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess(text: str) -> str:\n",
    "    \"\"\"Clean up the responses\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    import re\n",
    "    s = text\n",
    "    # Remove common control tokens\n",
    "    s = re.sub(r\"(###\\s*LLM_END###|###\\s*LLM_END\\s*###|LLM_END|<<<END>>>|<<END>>)\", \" \", s, flags=re.IGNORECASE)\n",
    "    # Strip leading markdown headers\n",
    "    s = re.sub(r\"^\\s*#{1,6}\\s*\", \"\", s, flags=re.MULTILINE)\n",
    "    # Trim trailing spaces before newlines\n",
    "    s = re.sub(r\"[ \\t]+\\n\", \"\\n\", s)\n",
    "    # Collapse 3+ newlines ‚Üí 2\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def build_evidence_pack(hits: List[Any], max_items: int = 6) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert Qdrant search hits into input to llm\n",
    "    Safely merges *_keywords arrays, normalizes text, and dedupes by report_key.\n",
    "    \"\"\"\n",
    "    import json, re\n",
    "\n",
    "    def _clean_text(s, max_len: int = 700) -> str:\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        s = str(s).replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s[:max_len]\n",
    "\n",
    "    def _to_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        if isinstance(x, list):\n",
    "            return [str(v) for v in x if v is not None]\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if s and s[0] in \"[{\":\n",
    "                try:\n",
    "                    parsed = json.loads(s)\n",
    "                    if isinstance(parsed, list):\n",
    "                        return [str(v) for v in parsed if v is not None]\n",
    "                    return [str(parsed)]\n",
    "                except Exception:\n",
    "                    return [x]\n",
    "            return [x]\n",
    "        try:\n",
    "            return [str(x)]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    seen = set()\n",
    "    pack: List[Dict[str, Any]] = []\n",
    "\n",
    "    for h in hits:\n",
    "        p = getattr(h, \"payload\", {}) or {}\n",
    "        rid_raw = p.get(\"report_key\", getattr(h, \"id\", None))\n",
    "        try:\n",
    "            rid = int(rid_raw)\n",
    "        except Exception:\n",
    "            rid = rid_raw\n",
    "\n",
    "        if rid in seen:\n",
    "            continue\n",
    "        seen.add(rid)\n",
    "\n",
    "        inj = _to_list(p.get(\"injury_keywords\"))\n",
    "        mal = _to_list(p.get(\"malfunction_keywords\"))\n",
    "        dea = _to_list(p.get(\"death_keywords\"))\n",
    "\n",
    "        # Merge keywords with simple de-duplication (preserve order)\n",
    "        mk_all: List[str] = []\n",
    "        for v in inj + mal + dea:\n",
    "            if v not in mk_all:\n",
    "                mk_all.append(v)\n",
    "\n",
    "        pack.append({\n",
    "            \"report_key\": rid,\n",
    "            \"manufacturer\": p.get(\"manufacturer\"),\n",
    "            \"year\": p.get(\"year\"),\n",
    "            \"event_type\": p.get(\"event_type\"),\n",
    "            \"predicted_label\": p.get(\"predicted_label\"),\n",
    "            \"final_label\": p.get(\"final_label\"),\n",
    "            \"final_score\": p.get(\"final_score\"),\n",
    "            \"adverse_event_flag\": p.get(\"adverse_event_flag\"),\n",
    "            \"matched_keywords\": mk_all,\n",
    "            \"summary\": _clean_text(p.get(\"summary\") or \"\"),\n",
    "            \"foi_text\": _clean_text(p.get(\"foi_text\") or \"\"),\n",
    "        })\n",
    "\n",
    "        if len(pack) >= max_items:\n",
    "            break\n",
    "\n",
    "    return pack\n",
    "\n",
    "STYLE_PRESETS = {\n",
    "    \"exec_summary\": {\n",
    "        \"tone\":  \"professional and concise\",\n",
    "        \"style\": \"bullet-led overview with short sentences\",\n",
    "        \"length\":\"150‚Äì220 words\",\n",
    "    },\n",
    "    \"technical\": {\n",
    "        \"tone\":  \"scientific, precise, and neutral\",\n",
    "        \"style\": \"well-structured paragraphs with explicit qualifiers\",\n",
    "        \"length\":\"220‚Äì350 words\",\n",
    "    },\n",
    "    \"support_reply\": {\n",
    "        \"tone\":  \"helpful and empathetic\",\n",
    "        \"style\": \"clear steps and recommendations\",\n",
    "        \"length\":\"150‚Äì220 words\",\n",
    "    },\n",
    "    \"detailed\": {\n",
    "        \"tone\":  \"balanced and explanatory\",\n",
    "        \"style\": \"2‚Äì3 short paragraphs plus bullets\",\n",
    "        \"length\":\"220‚Äì320 words\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def make_prompt(question: str,\n",
    "                role: str,\n",
    "                audience: str,\n",
    "                evidence_pack: list[dict],\n",
    "                preset: str = \"detailed\") -> str:\n",
    "    \"\"\"Build a grounded prompt that tailors tone/wording to the given audience.\"\"\"\n",
    "    cfg = STYLE_PRESETS.get(preset, STYLE_PRESETS[\"detailed\"])\n",
    "\n",
    "    # Flatten evidence into readable blocks\n",
    "    blocks = []\n",
    "    for e in evidence_pack:\n",
    "        header = f\"[#{e.get('report_key','?')}] {e.get('manufacturer','?')} ¬∑ {e.get('year','?')} ¬∑ {e.get('event_type','?')} ¬∑ final={e.get('final_label','?')}\"\n",
    "        body = (e.get(\"summary\") or e.get(\"foi_text\") or \"\").strip()\n",
    "        mk = e.get(\"matched_keywords\") or []\n",
    "        if mk:\n",
    "            body += \" | Matched keywords: \" + \", \".join(sorted({str(x) for x in mk}))\n",
    "        blocks.append(f\"{header}\\n{body}\")\n",
    "    evid = \"\\n\\n\".join(blocks) if blocks else \"No matching evidence snippets found.\"\n",
    "\n",
    "    system_instructions = f\"\"\"\n",
    "You are an expert {role}. Write for a {audience} audience.\n",
    "Tone: {cfg['tone']}. Style: {cfg['style']}. Target length: {cfg['length']}.\n",
    "\n",
    "Critical rules:\n",
    "- ANSWER ONLY using the evidence snippets below. Do NOT invent facts.\n",
    "- Attribute every factual claim with inline citations like [#17002669] using the report_key.\n",
    "- Prefer precise language and short sentences.\n",
    "- If evidence is weak or mixed, say so and qualify conclusions.\n",
    "- Keep manufacturer names and years exactly as stated. Preserve domain terms.\n",
    "- Use bullets when listing causes, counts, or examples.\n",
    "- Do NOT output control tokens (###, JSON, XML) unless asked.\n",
    "\"\"\".strip()\n",
    "\n",
    "    return f\"\"\"{system_instructions}\n",
    "\n",
    "Question: {question.strip()}\n",
    "\n",
    "Evidence Snippets:\n",
    "{evid}\n",
    "\n",
    "Write the final answer now.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_with_ollama(prompt: str, model: str, temperature: float, num_predict: int, url: str) -> str:\n",
    "    \"\"\"Call a local Ollama model and return the generated text.\"\"\"\n",
    "    try:\n",
    "        r = requests.post(\n",
    "            f\"{url}/api/generate\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"options\": {\n",
    "                    \"temperature\": float(temperature),\n",
    "                    \"num_predict\": int(num_predict)\n",
    "                },\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=180\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return postprocess(data.get(\"response\", \"\") or \"\")\n",
    "    except Exception as e:\n",
    "        return f\"‚ö†Ô∏è Error calling Ollama: {e}\"\n",
    "\n",
    "\n",
    "def generate_answer(hits, question: str, role: str, audience: str, preset: str,\n",
    "                    model: str, url: str, temperature: float, num_predict: int) -> str:\n",
    "    \"\"\"\n",
    "    Turn retrieved Qdrant hits into an evidence pack, build a grounded prompt,\n",
    "    and get the final answer from Ollama. Appends a plain ID list for easy copy/paste.\n",
    "    \"\"\"\n",
    "    pack = build_evidence_pack(hits, max_items=6)\n",
    "    prompt = make_prompt(question, role, audience, pack, preset)  # <-- now audience-aware\n",
    "    out = generate_with_ollama(prompt, model=model, temperature=temperature,\n",
    "                               num_predict=num_predict, url=url)\n",
    "\n",
    "    ids, seen = [], set()\n",
    "    for e in pack:\n",
    "        rk = e.get(\"report_key\")\n",
    "        if rk is not None and rk not in seen:\n",
    "            ids.append(str(rk)); seen.add(rk)\n",
    "    if ids:\n",
    "        out += f\"\\n\\nIDs: {', '.join(ids)}\"\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule basedintent & parsing \n",
    "import re\n",
    "\n",
    "EVENT_TYPES = {\"Injury\", \"Device Malfunction\", \"Death\"}\n",
    "LABELS      = {\"Injury\", \"Device Malfunction\", \"Death\"}\n",
    "CONTRADICT_WORDS = {\"mismatch\", \"disagree\", \"contradiction\", \"contradict\", \"vs\", \"different\", \"conflict\"}\n",
    "\n",
    "# Years: 2000‚Äì2049 in range instead of one year\n",
    "YEAR_RE_SINGLE = re.compile(r\"\\b(20[0-4]\\d)\\b\")\n",
    "YEAR_RE_RANGE  = re.compile(r\"\\b(20[0-4]\\d)\\s*[-‚Äì]\\s*(20[0-4]\\d)\\b\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip().lower()\n",
    "\n",
    "# Liberal detector for ‚Äúlist manufacturers‚Äù variants\n",
    "LIST_MFG_RE = re.compile(\n",
    "    r\"\\b(list|show|display|give|provide|names?\\s+of|which|who)\\b.*\\b(manufacturers?|manufacturer\\s+names?)\\b\",\n",
    "    re.I,\n",
    ")\n",
    "ALT_LIST_TOKENS = {\"list\", \"show\", \"display\", \"give\", \"provide\", \"name\", \"names\", \"all\", \"which\", \"who\"}\n",
    "\n",
    "def is_list_manufacturers_query(q: str) -> bool:\n",
    "    qn = norm(q)\n",
    "    return bool(LIST_MFG_RE.search(qn) or\n",
    "                ((\"manufacturer\" in qn or \"manufacturers\" in qn) and any(t in qn for t in ALT_LIST_TOKENS)))\n",
    "\n",
    "# Detector for ‚Äúcount reports‚Äù variants\n",
    "COUNT_RE = re.compile(r\"\\b(how\\s*many|count|number\\s+of)\\b.*\\breports?\\b\", re.I)\n",
    "def is_count_reports_query(q: str) -> bool:\n",
    "    return COUNT_RE.search(norm(q)) is not None\n",
    "\n",
    "# Quoted phrases ‚Üí exact payload MatchAny on *_keywords arrays\n",
    "def parse_quoted_keywords(q: str):\n",
    "    return [m.group(1).strip().lower() for m in re.finditer(r'\"([^\"]+)\"', q)]\n",
    "\n",
    "# Extract explicit years / ranges\n",
    "def extract_years(q: str):\n",
    "    years = [int(y) for y in YEAR_RE_SINGLE.findall(q)]\n",
    "    for a, b in YEAR_RE_RANGE.findall(q):\n",
    "        years.extend([int(a), int(b)])\n",
    "    return sorted(set(years))\n",
    "\n",
    "def embed(text: str) -> List[float]:\n",
    "    return encoder.encode([text], normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "def infer_constraints(q: str) -> Dict[str, Any]:\n",
    "    qn = norm(q)\n",
    "    c: Dict[str, Any] = {\"manufacturer\": None, \"event_type\": None, \"label\": None,\n",
    "                         \"years\": [], \"keyword_hint\": None, \"mode\": None}\n",
    "    c[\"years\"] = extract_years(q)\n",
    "\n",
    "    # AE type/label detection\n",
    "    for et in EVENT_TYPES:\n",
    "        if et.lower() in qn:\n",
    "            c[\"event_type\"] = et\n",
    "    for lb in LABELS:\n",
    "        if lb.lower() in qn and c[\"event_type\"] is None:\n",
    "            c[\"label\"] = lb\n",
    "\n",
    "    # contradiction mode\n",
    "    if any(w in qn for w in CONTRADICT_WORDS):\n",
    "        c[\"mode\"] = \"contradiction\"\n",
    "\n",
    "    # naive manufacturer capture (kept from your version)\n",
    "    m = re.search(r\"\\b([A-Z][a-zA-Z]+(?: [A-Z][a-zA-Z]+){0,3})\\b\", q)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        if cand.lower() not in {\"which\",\"show\",\"list\",\"reports\",\"devices\",\"manufacturer\"}:\n",
    "            c[\"manufacturer\"] = cand\n",
    "\n",
    "    # fallback keyword hint = entire question\n",
    "    c[\"keyword_hint\"] = q.strip()\n",
    "    return c\n",
    "\n",
    "# ================== Qdrant callings==================\n",
    "def _scroll_compat(client, collection: str, flt, limit: int, offset=None, with_payload=True):\n",
    "    \"\"\"Handle scroll_filter/filter param name across client versions.\"\"\"\n",
    "    try:\n",
    "        return client.scroll(\n",
    "            collection_name=collection,\n",
    "            with_payload=with_payload,\n",
    "            limit=limit,\n",
    "            offset=offset,\n",
    "            scroll_filter=flt, \n",
    "        )\n",
    "    except (AssertionError, TypeError):\n",
    "        return client.scroll(\n",
    "            collection_name=collection,\n",
    "            with_payload=with_payload,\n",
    "            limit=limit,\n",
    "            offset=offset,\n",
    "            filter=flt,       \n",
    "        )\n",
    "\n",
    "def _count_compat(client, collection: str, flt):\n",
    "    \"\"\"Handle count_filter/filter param name across client versions.\"\"\"\n",
    "    try:\n",
    "        return client.count(collection_name=collection, exact=True, count_filter=flt).count\n",
    "    except (AssertionError, TypeError):\n",
    "        return client.count(collection_name=collection, exact=True, filter=flt).count\n",
    "\n",
    "# ================== Filter builders ==================\n",
    "\n",
    "\n",
    "# ----------------------- new filter -------\n",
    "\n",
    "\n",
    "\n",
    "def _norm_many(v) -> List[str]:\n",
    "    \"\"\"Normalize a single value or an iterable to a clean list of non-empty strings.\"\"\"\n",
    "    if v is None:\n",
    "        return []\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        it = v\n",
    "    else:\n",
    "        it = [v]\n",
    "    out = []\n",
    "    for x in it:\n",
    "        if x is None:\n",
    "            continue\n",
    "        s = str(x).strip()\n",
    "        if s:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "def _coerce_int_list(v) -> List[int]:\n",
    "    if v is None:\n",
    "        return []\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        it = v\n",
    "    else:\n",
    "        it = [v]\n",
    "    out = []\n",
    "    for x in it:\n",
    "        try:\n",
    "            out.append(int(x))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def build_filter(c: Dict[str, Any], payload_year_key: str = \"year\") -> Optional[Filter]:\n",
    "    \"\"\"\n",
    "    Backward-compatible filter builder with optional extras:\n",
    "      - manufacturer/event_type/label accept str or list[str]\n",
    "      - years supports single year, small set, or a range (same logic as before)\n",
    "      - report_key / report_keys (exact or list[int])\n",
    "      - keywords (list[str]) matched across injury/malfunction/death keyword arrays\n",
    "    \"\"\"\n",
    "    must: List[Any] = []\n",
    "\n",
    "    # --- manufacturer ---\n",
    "    manufacturers = _norm_many(c.get(\"manufacturer\"))\n",
    "    if manufacturers:\n",
    "        if len(manufacturers) == 1:\n",
    "            must.append(FieldCondition(key=\"manufacturer\", match=MatchValue(value=manufacturers[0])))\n",
    "        else:\n",
    "            must.append(FieldCondition(key=\"manufacturer\", match=MatchAny(any=manufacturers)))\n",
    "\n",
    "    # --- event_type ---\n",
    "    event_types = _norm_many(c.get(\"event_type\"))\n",
    "    if event_types:\n",
    "        if len(event_types) == 1:\n",
    "            must.append(FieldCondition(key=\"event_type\", match=MatchValue(value=event_types[0])))\n",
    "        else:\n",
    "            must.append(FieldCondition(key=\"event_type\", match=MatchAny(any=event_types)))\n",
    "\n",
    "    # --- label -> final_label (keeps original semantics) ---\n",
    "    labels = _norm_many(c.get(\"label\"))\n",
    "    if labels:\n",
    "        if len(labels) == 1:\n",
    "            must.append(FieldCondition(key=\"final_label\", match=MatchValue(value=labels[0])))\n",
    "        else:\n",
    "            must.append(FieldCondition(key=\"final_label\", match=MatchAny(any=labels)))\n",
    "\n",
    "    # --- years (original logic preserved) ---\n",
    "    years = _coerce_int_list(c.get(\"years\", []))\n",
    "    if years:\n",
    "        if len(years) == 1:\n",
    "            must.append(FieldCondition(key=payload_year_key, match=MatchValue(value=years[0])))\n",
    "        elif len(years) == 2 and (max(years) - min(years) > 1):\n",
    "            lo, hi = min(years), max(years)\n",
    "            must.append(FieldCondition(key=payload_year_key, range=Range(gte=lo, lte=hi)))\n",
    "        else:\n",
    "            must.append(FieldCondition(key=payload_year_key, match=MatchAny(any=years)))\n",
    "\n",
    "    # --- report_key (optional: exact or list) ---\n",
    "    report_keys = _coerce_int_list(c.get(\"report_keys\", c.get(\"report_key\")))\n",
    "    if report_keys:\n",
    "        if len(report_keys) == 1:\n",
    "            must.append(FieldCondition(key=\"report_key\", match=MatchValue(value=report_keys[0])))\n",
    "        else:\n",
    "            must.append(FieldCondition(key=\"report_key\", match=MatchAny(any=report_keys)))\n",
    "\n",
    "    # --- keywords (optional): match across all three keyword arrays ---\n",
    "    keywords = [s.lower() for s in _norm_many(c.get(\"keywords\"))]\n",
    "    if keywords:\n",
    "        must.append(\n",
    "            Filter(\n",
    "                should=[\n",
    "                    FieldCondition(key=\"injury_keywords\",      match=MatchAny(any=keywords)),\n",
    "                    FieldCondition(key=\"malfunction_keywords\", match=MatchAny(any=keywords)),\n",
    "                    FieldCondition(key=\"death_keywords\",       match=MatchAny(any=keywords)),\n",
    "                ],\n",
    "                minimum_should_match=1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return Filter(must=must) if must else None\n",
    "\n",
    "\n",
    "# Add OR logic to keyword filters \n",
    "def add_keyword_filters(base: Optional[Filter], kw_terms: List[str]) -> Optional[Filter]:\n",
    "    if not kw_terms:\n",
    "        return base\n",
    "    should = [\n",
    "        FieldCondition(key=\"injury_keywords\",      match=MatchAny(any=kw_terms)),\n",
    "        FieldCondition(key=\"malfunction_keywords\", match=MatchAny(any=kw_terms)),\n",
    "        FieldCondition(key=\"death_keywords\",       match=MatchAny(any=kw_terms)),\n",
    "    ]\n",
    "    if base is None:\n",
    "        return Filter(should=should)\n",
    "    return Filter(must=list(base.must or []), should=should, must_not=base.must_not)\n",
    "\n",
    "\n",
    "def facet_counts(collection: str, field: str, flt: Optional[Filter] = None):\n",
    "    counts, offset = {}, None\n",
    "    while True:\n",
    "        points, offset = _scroll_compat(client, collection, flt, limit=256, offset=offset, with_payload=True)\n",
    "        if not points:\n",
    "            break\n",
    "        for p in points:\n",
    "            pay = getattr(p, \"payload\", {}) or {}\n",
    "            v = pay.get(field)\n",
    "            if isinstance(v, list):\n",
    "                for x in v:\n",
    "                    if x is not None:\n",
    "                        counts[x] = counts.get(x, 0) + 1\n",
    "            elif v is not None:\n",
    "                counts[v] = counts.get(v, 0) + 1\n",
    "        if offset is None:\n",
    "            break\n",
    "    return dict(sorted(counts.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "\n",
    "\n",
    "def resolve_manufacturer(collection: str, name: str) -> Optional[str]:\n",
    "    if not name:\n",
    "        return None\n",
    "    qvec = embed(name)\n",
    "    hits = client.search(collection_name=collection, query_vector=(\"mfg\", qvec), limit=3, with_payload=True)\n",
    "    if not hits:\n",
    "        return None\n",
    "    return hits[0].payload.get(\"manufacturer\")\n",
    "\n",
    "def semantic_search(collection: str, text: str, limit: int = TOP_K,\n",
    "                    flt: Optional[Filter] = None, vector_name: str = \"doc\"):\n",
    "    qvec = embed(text)\n",
    "    return client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=(vector_name, qvec),\n",
    "        query_filter=flt,\n",
    "        with_payload=True,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "def filter_only(collection: str, flt: Filter, limit: int = 200):\n",
    "    dim = len(embed(\"dim-probe\"))\n",
    "    zero = [0.0] * dim\n",
    "    return client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=(\"doc\", zero),\n",
    "        query_filter=flt,\n",
    "        with_payload=True,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "def contradiction_hits(collection: str, c: Dict[str, Any]):\n",
    "    must: List[Any] = []\n",
    "    if c.get(\"manufacturer\"):\n",
    "        must.append(FieldCondition(key=\"manufacturer\", match=MatchValue(value=c[\"manufacturer\"])))\n",
    "    years = _coerce_int_list(c.get(\"years\", []))\n",
    "    if years:\n",
    "        if len(years) == 1:\n",
    "            must.append(FieldCondition(key=\"year\", match=MatchValue(value=years[0])))\n",
    "        else:\n",
    "            lo, hi = min(years), max(years)\n",
    "            must.append(FieldCondition(key=\"year\", range=Range(gte=lo, lte=hi)))\n",
    "    patterns = [\n",
    "        Filter(must=must + [\n",
    "            FieldCondition(key=\"event_type\",  match=MatchValue(value=\"Device Malfunction\")),\n",
    "            FieldCondition(key=\"final_label\", match=MatchValue(value=\"Injury\")),\n",
    "        ]),\n",
    "        Filter(must=must + [\n",
    "            FieldCondition(key=\"event_type\",  match=MatchValue(value=\"Injury\")),\n",
    "            FieldCondition(key=\"final_label\", match=MatchValue(value=\"Device Malfunction\")),\n",
    "        ]),\n",
    "        Filter(must=must + [\n",
    "            FieldCondition(key=\"adverse_event_flag\", match=MatchValue(value=\"N\")),\n",
    "            FieldCondition(key=\"final_label\",        match=MatchValue(value=\"Injury\")),\n",
    "        ]),\n",
    "    ]\n",
    "    out = []\n",
    "    for flt in patterns:\n",
    "        out.extend(filter_only(collection, flt, limit=200))\n",
    "    return out\n",
    "\n",
    "# ================== Search Route rule based==================\n",
    "def search_router(collection: str, user_query: str,\n",
    "                  min_hits: int = MIN_HITS, max_hits: int = MAX_HITS):\n",
    "    c = infer_constraints(user_query)\n",
    "    if c.get(\"manufacturer\"):\n",
    "        resolved = resolve_manufacturer(collection, c[\"manufacturer\"])\n",
    "        if resolved:\n",
    "            c[\"manufacturer\"] = resolved\n",
    "\n",
    "    # contradiction mode\n",
    "    if c.get(\"mode\") == \"contradiction\":\n",
    "        hits = contradiction_hits(collection, c)\n",
    "        if not hits:\n",
    "            hits = semantic_search(collection, c[\"keyword_hint\"], limit=max_hits)\n",
    "        return hits\n",
    "\n",
    "    # base filters\n",
    "    flt = build_filter(c)\n",
    "\n",
    "    # quoted keyword filters\n",
    "    kw_terms = [k.lower() for k in parse_quoted_keywords(user_query)]\n",
    "    if kw_terms:\n",
    "        flt = add_keyword_filters(flt, kw_terms)\n",
    "\n",
    "    # filtered semantic\n",
    "    if flt:\n",
    "        hits = semantic_search(collection, c[\"keyword_hint\"], limit=max_hits, flt=flt, vector_name=\"doc\")\n",
    "        if len(hits) < min_hits:\n",
    "            fonly = filter_only(collection, flt, limit=200)\n",
    "            if fonly:\n",
    "                seen = {h.id for h in hits}\n",
    "                for h in fonly:\n",
    "                    if h.id not in seen:\n",
    "                        hits.append(h)\n",
    "        if len(hits) < min_hits and c.get(\"label\"):\n",
    "            c2 = dict(c); c2[\"label\"] = None\n",
    "            flt2 = build_filter(c2)\n",
    "            if kw_terms:\n",
    "                flt2 = add_keyword_filters(flt2, kw_terms)\n",
    "            if flt2:\n",
    "                hits = semantic_search(collection, c[\"keyword_hint\"], limit=max_hits, flt=flt2)\n",
    "        if len(hits) < min_hits and c.get(\"years\"):\n",
    "            lo, hi = min(c[\"years\"]), max(c[\"years\"])\n",
    "            c3 = dict(c); c3[\"years\"] = [lo-1, hi+1]\n",
    "            flt3 = build_filter(c3)\n",
    "            if kw_terms:\n",
    "                flt3 = add_keyword_filters(flt3, kw_terms)\n",
    "            if flt3:\n",
    "                hits = semantic_search(collection, c[\"keyword_hint\"], limit=max_hits, flt=flt3)\n",
    "        if hits:\n",
    "            return hits\n",
    "\n",
    "    # plain semantic search\n",
    "    hits = semantic_search(collection, c[\"keyword_hint\"], limit=max_hits, vector_name=\"doc\")\n",
    "    if hits:\n",
    "        return hits\n",
    "\n",
    "    # manufacturer vector search as last resort\n",
    "    if \"manufacturer\" in norm(user_query):\n",
    "        return semantic_search(collection, user_query, limit=max_hits, vector_name=\"mfg\")\n",
    "    return []\n",
    "\n",
    "# ================== Listings & counts type queries ==================\n",
    "def ask_question(question: str, role: str, audience: str, preset: str,\n",
    "                 model: str, url: str, temperature: float, num_predict: int) -> str:\n",
    "\n",
    "    # (A) List manufacturers (+counts), respecting AE type / year facets\n",
    "    if is_list_manufacturers_query(question):\n",
    "        c = infer_constraints(question)\n",
    "        # CRITICAL: drop manufacturer facet so we truly list *all* manufacturers under other filters\n",
    "        c[\"manufacturer\"] = None\n",
    "        flt = build_filter(c)\n",
    "        cnt = facet_counts(COLLECTION, \"manufacturer\", flt)\n",
    "        if not cnt:\n",
    "            return \"No manufacturers found for the requested filters.\"\n",
    "        lines = [f\"‚Ä¢ {m} ({n} reports)\" for m, n in cnt.items()]\n",
    "        return \"Manufacturers in the index:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "    # (B) Count reports\n",
    "    if is_count_reports_query(question):\n",
    "        c   = infer_constraints(question)\n",
    "        flt = build_filter(c)\n",
    "        total = _count_compat(client, COLLECTION, flt)\n",
    "        return f\"Total reports matching your filters: {total}\"\n",
    "\n",
    "    # (C) Default RAG answer\n",
    "    hits = search_router(COLLECTION, question, min_hits=MIN_HITS, max_hits=MAX_HITS)\n",
    "    if not hits:\n",
    "        return \"No matching evidence found in the index.\"\n",
    "    return generate_answer(hits, question, role, audience, preset, model, url, temperature, num_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "try:\n",
    "    ae_types = [\"Injury\", \"Device Malfunction\", \"Death\"]\n",
    "    years_list = sorted(df[\"year\"].dropna().astype(int).unique().tolist())\n",
    "    manufacturers_list = sorted(df[\"manufacturer\"].dropna().astype(str).unique().tolist())\n",
    "\n",
    "    def _safe_kw(col):\n",
    "        out = []\n",
    "        if col in df.columns:\n",
    "            for v in df[col].dropna().tolist():\n",
    "                if isinstance(v, list):\n",
    "                    out.extend(v)\n",
    "                else:\n",
    "                    # strings like '[\"a\",\"b\"]' or 'a; b'\n",
    "                    try:\n",
    "                        import json, re\n",
    "                        if isinstance(v, str) and v.strip().startswith((\"[\",\"{\")):\n",
    "                            vv = json.loads(v)\n",
    "                            if isinstance(vv, list): out.extend(vv)\n",
    "                        else:\n",
    "                            for tok in re.split(r\"[;,\\|\\n]+\", str(v)):\n",
    "                                tok = tok.strip()\n",
    "                                if tok: out.append(tok)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        return out\n",
    "\n",
    "    keywords_list = sorted({k.strip().lower()\n",
    "                            for k in (_safe_kw(\"injury_keywords\")\n",
    "                                      + _safe_kw(\"malfunction_keywords\")\n",
    "                                      + _safe_kw(\"death_keywords\"))\n",
    "                            if k})\n",
    "except Exception:\n",
    "    # fallback if df not in scope\n",
    "    ae_types = [\"Injury\", \"Device Malfunction\", \"Death\"]\n",
    "    years_list = []\n",
    "    manufacturers_list = []\n",
    "    keywords_list = []\n",
    "\n",
    "# ----- Chatbot UI wrapper around ask_question call -----\n",
    "\n",
    "def build_gradio_chat():\n",
    "    def _build_filters(\n",
    "        sel_labels: List[str],\n",
    "        sel_years: List[int],\n",
    "        sel_mfgs: List[str],\n",
    "        report_key: str,\n",
    "        sel_keywords: List[str],\n",
    "    ) -> Dict[str, Any]:\n",
    "        c: Dict[str, Any] = {}\n",
    "        if sel_mfgs:   c[\"manufacturer\"] = sel_mfgs\n",
    "        if sel_labels: c[\"label\"] = sel_labels\n",
    "        if sel_years:  c[\"years\"] = sel_years\n",
    "        if report_key and str(report_key).strip():\n",
    "            # accept single or comma-separated\n",
    "            try:\n",
    "                ids = [int(x) for x in str(report_key).replace(\" \", \"\").split(\",\") if x]\n",
    "                c[\"report_keys\"] = ids if len(ids) > 1 else ids[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "        if sel_keywords: c[\"keywords\"] = [k.strip().lower() for k in sel_keywords]\n",
    "        return c\n",
    "\n",
    "    def chat_step(history, message, role, audience, preset, temperature, num_predict,\n",
    "                  sel_labels, sel_years, sel_mfgs, report_key, sel_keywords):\n",
    "        history = history or []\n",
    "        msg = (message or \"\").strip()\n",
    "        if not msg:\n",
    "            return history, \"\"\n",
    "\n",
    "        # Show user message immediately\n",
    "        history.append((msg, None))\n",
    "\n",
    "        filters = _build_filters(sel_labels, sel_years, sel_mfgs, report_key, sel_keywords)\n",
    "\n",
    "        try:\n",
    "            # Prefer calling ask_question with filters if supported\n",
    "            reply = ask_question(\n",
    "                msg,\n",
    "                role=role,\n",
    "                audience=audience,\n",
    "                preset=preset,\n",
    "                model=OLLAMA_MODEL,\n",
    "                url=OLLAMA_URL,\n",
    "                temperature=float(temperature),\n",
    "                num_predict=int(num_predict),\n",
    "                filters=filters,                 \n",
    "            )\n",
    "        except TypeError:\n",
    "\n",
    "            reply = ask_question(\n",
    "                msg,\n",
    "                role=role,\n",
    "                audience=audience,\n",
    "                preset=preset,\n",
    "                model=OLLAMA_MODEL,\n",
    "                url=OLLAMA_URL,\n",
    "                temperature=float(temperature),\n",
    "                num_predict=int(num_predict),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            reply = f\"‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "        # Fill assistant reply\n",
    "        history[-1] = (msg, reply)\n",
    "        return history, \"\"  # clear input box\n",
    "\n",
    "    with gr.Blocks(title=\"Qdrant + Ollama RAG Chatbot\") as demo:\n",
    "        gr.Markdown(\"## üßë‚Äç‚öïÔ∏è Medical Device RAG Chatbot\\nAsk natural questions; answers cite report IDs like `[#17002669]` where applicable.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(height=520)\n",
    "                user_box = gr.Textbox(\n",
    "                    placeholder='Examples: list manufacturers; count Injury in 2024; reports with \"vascular laceration\" in 2021.',\n",
    "                    lines=2,\n",
    "                    label=\"Your question\"\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Search & Answer\", variant=\"primary\")\n",
    "                    clear_btn  = gr.Button(\"Clear\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"### Filters (optional)\")\n",
    "                sel_labels = gr.CheckboxGroup(choices=ae_types, label=\"AE type (final_label)\")\n",
    "                sel_years = gr.Dropdown(choices=years_list, multiselect=True, label=\"Year(s)\")\n",
    "                sel_mfgs = gr.Dropdown(choices=manufacturers_list, multiselect=True, label=\"Manufacturer(s)\")\n",
    "                report_key = gr.Textbox(label=\"Report key(s) (comma-separated)\")\n",
    "                sel_keywords = gr.Dropdown(choices=keywords_list, multiselect=True, label=\"Keywords\")\n",
    "\n",
    "                gr.Markdown(\"### Settings\")\n",
    "                role        = gr.Textbox(label=\"Role\", value=\"clinical safety analyst\")\n",
    "                audience    = gr.Textbox(label=\"Audience\", value=\"scientifically literate\")\n",
    "                preset      = gr.Dropdown(\n",
    "                    choices=[\"exec_summary\",\"technical\",\"detailed\",\"support_reply\"],\n",
    "                    value=\"exec_summary\",\n",
    "                    label=\"Style preset\"\n",
    "                )\n",
    "                temperature = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Temperature\")\n",
    "                num_predict = gr.Slider(128, 2048, value=700, step=32, label=\"Max tokens (num_predict)\")\n",
    "\n",
    "        # Wire events\n",
    "        submit_btn.click(\n",
    "            fn=chat_step,\n",
    "            inputs=[chatbot, user_box, role, audience, preset, temperature, num_predict,\n",
    "                    sel_labels, sel_years, sel_mfgs, report_key, sel_keywords],\n",
    "            outputs=[chatbot, user_box],\n",
    "        )\n",
    "        user_box.submit(\n",
    "            fn=chat_step,\n",
    "            inputs=[chatbot, user_box, role, audience, preset, temperature, num_predict,\n",
    "                    sel_labels, sel_years, sel_mfgs, report_key, sel_keywords],\n",
    "            outputs=[chatbot, user_box],\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            fn=lambda: ([], \"\"),\n",
    "            inputs=None,\n",
    "            outputs=[chatbot, user_box],\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "demo = build_gradio_chat()\n",
    "demo.launch()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e230f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Test runner for your ask_question(...) ----\n",
    "# Assumes ask_question, client, encoder, COLLECTION, OLLAMA_URL, OLLAMA_MODEL,\n",
    "# and TOP_K/MIN_HITS/MAX_HITS are already defined/imported elsewhere.\n",
    "\n",
    "# def run(q: str,\n",
    "#         role=\"clinical safety analyst\",\n",
    "#         audience=\"scientifically literate\",\n",
    "#         preset=\"exec_summary\",\n",
    "#         model=OLLAMA_MODEL,\n",
    "#         url=OLLAMA_URL,\n",
    "#         temperature=0.2,\n",
    "#         num_predict=700):\n",
    "#     print(\"\\n\" + \"=\"*90)\n",
    "#     print(\">>>\", q)\n",
    "#     try:\n",
    "#         ans = ask_question(\n",
    "#             q, role=role, audience=audience, preset=preset,\n",
    "#             model=model, url=url, temperature=temperature, num_predict=num_predict\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         ans = f\"‚ö†Ô∏è Error: {e}\"\n",
    "#     print(ans)\n",
    "\n",
    "# QUERIES = [\n",
    "#     # A) List manufacturers (facet listing)\n",
    "#     \"list all manufacturers\",\n",
    "#     \"show manufacturer names for injury\",\n",
    "#     \"list manufacturers in 2023\",\n",
    "#     \"list all manufacturers for device malfunction in 2022‚Äì2024\",\n",
    "\n",
    "#     # B) Count reports\n",
    "#     \"how many reports in 2023\",\n",
    "#     \"count reports for injury in 2024\",\n",
    "#     \"number of reports for device malfunction in 2022-2024\",\n",
    "#     \"how many death reports in 2022\",\n",
    "#     'count reports for \"balloon leak\" in 2023-2024',\n",
    "\n",
    "#     # C) Quoted keyword filters (exact MatchAny on *_keywords)\n",
    "#     'show reports mentioning \"balloon leak\"',\n",
    "#     'list manufacturers for \"kinked sheath\" in 2024',\n",
    "#     'count reports with \"catheter jammed\" in 2023',\n",
    "#     'retrieve evidence about \"cuff miss\" in 2022‚Äì2023',\n",
    "\n",
    "#     # D) Contradiction / audit mode\n",
    "#     \"find contradiction between event type and final label\",\n",
    "#     \"device malfunction vs injury contradictions for Essential Medical Inc in 2023\",\n",
    "\n",
    "#     # E) Manufacturer resolution & filtered retrieval\n",
    "#     \"show reports from Essential Medical Inc in 2024\",\n",
    "#     \"injury reports by Abbott Vascular Reg in 2023\",\n",
    "\n",
    "#     # F) General semantic retrieval\n",
    "#     \"show evidence of sheath buckling related malfunctions\",\n",
    "#     \"summarize common causes of incomplete closure\",\n",
    "#     \"incidents related to balloon deflation\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# for q in QUERIES:\n",
    "#        run(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565e997",
   "metadata": {},
   "source": [
    "## Chatbot with new foi text analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237e1025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "# ================== Imports & Globals ==================\n",
    "import json, re, numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from keybert import KeyBERT\n",
    "import gradio as gr\n",
    "\n",
    "# ------------------- Your constants -------------------\n",
    "LABELS = [\"Death\", \"Injury\", \"Device Malfunction\"]\n",
    "\n",
    "HI_THRESH = 0.70\n",
    "MARGIN_THRESH = 0.10\n",
    "ALPHA_DEFAULT = 0.60\n",
    "ALPHA_LOW = 0.35\n",
    "\n",
    "EVIDENCE_TOPK = 12\n",
    "EVIDENCE_MIN_COS = 0.30\n",
    "\n",
    "ENFORCE_DEATH_CONFIRMATION = True\n",
    "DEATH_EVID_MIN = 0.55\n",
    "DEATH_COS_MIN = 0.58\n",
    "EVIDENCE_CAP_NO_STRICT_DEATH = 0.35\n",
    "\n",
    "DEATH_STRICT = [\n",
    "    r\"\\bpatient (?:died|expired)\\b\", r\"\\bpronounced dead\\b\", r\"\\bdeclared dead\\b\",\n",
    "    r\"\\bfound deceased\\b\", r\"\\bpassed away\\b\",\n",
    "    r\"\\bfatal (?:event|outcome|complication)\\b\",\n",
    "    r\"\\bdeath (?:occurred|reported|confirmed)\\b\",\n",
    "    r\"\\bmortality\\b\", r\"\\bfatality\\b\"\n",
    "]\n",
    "NEGATION_PAT = r\"(no|not|without|never|denies?|rule[sd]?\\s*out)\\s+(?:any\\s+)?(death|died|deceased|fatal|expired)\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_prob_dict(x):\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return json.loads(x.replace(\"'\", '\"'))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def has_strict_death(text: str) -> bool:\n",
    "    if re.search(NEGATION_PAT, text, flags=re.I):\n",
    "        return False\n",
    "    return any(re.search(pat, text, flags=re.I) for pat in DEATH_STRICT)\n",
    "\n",
    "def keybert_terms(text, top_k=15):\n",
    "    kws = kw_model.extract_keywords(\n",
    "        text, keyphrase_ngram_range=(1,3), stop_words='english', top_n=top_k\n",
    "    )\n",
    "    return [w for w,_ in kws]\n",
    "\n",
    "def evidence_for_label(text, label):\n",
    "    if label not in DICT_TERMS:\n",
    "        return 0.0, []\n",
    "\n",
    "    ext_terms = keybert_terms(text, top_k=EVIDENCE_TOPK)\n",
    "    if not ext_terms:\n",
    "        return 0.0, []\n",
    "\n",
    "    dict_terms = DICT_TERMS[label]\n",
    "    dict_emb = bert_model.encode(dict_terms, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    ext_emb  = bert_model.encode(ext_terms,  convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    cos = util.cos_sim(ext_emb, dict_emb).cpu().numpy()\n",
    "\n",
    "    matches = []\n",
    "    min_cos = DEATH_COS_MIN if label == \"Death\" else EVIDENCE_MIN_COS\n",
    "\n",
    "    for i, term in enumerate(ext_terms):\n",
    "        j = int(cos[i].argmax())\n",
    "        s = float(cos[i][j])\n",
    "        if s >= min_cos:\n",
    "            matches.append({\n",
    "                \"extracted_term\": term,\n",
    "                \"matched_dict_term\": dict_terms[j],\n",
    "                \"score\": round(s, 3)\n",
    "            })\n",
    "\n",
    "    # Always include fallback matches for Injury and Device Malfunction\n",
    "    if label in [\"Injury\", \"Device Malfunction\"]:\n",
    "        for i, term in enumerate(ext_terms):\n",
    "            j = int(cos[i].argmax())\n",
    "            s = float(cos[i][j])\n",
    "            if s >= 0.20 and not any(m[\"extracted_term\"] == term for m in matches):\n",
    "                matches.append({\n",
    "                    \"extracted_term\": term,\n",
    "                    \"matched_dict_term\": dict_terms[j],\n",
    "                    \"score\": round(s, 3)\n",
    "                })\n",
    "\n",
    "    if not matches:\n",
    "        if label == \"Death\" and has_strict_death(text):\n",
    "            return 0.9, []\n",
    "        return 0.0, []\n",
    "\n",
    "    top_scores = sorted([m[\"score\"] for m in matches], reverse=True)[:5]\n",
    "    evid = float(np.mean(top_scores))\n",
    "\n",
    "    if label == \"Death\":\n",
    "        if has_strict_death(text):\n",
    "            evid = max(evid, 0.9)\n",
    "        else:\n",
    "            evid = min(evid, EVIDENCE_CAP_NO_STRICT_DEATH)\n",
    "\n",
    "    return evid, sorted(matches, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "def clean_keyword_list(matches, topn=8, with_scores=False):\n",
    "    seen, cleaned = set(), []\n",
    "    for m in sorted(matches, key=lambda x: x[\"score\"], reverse=True):\n",
    "        term = m[\"matched_dict_term\"]\n",
    "        if term in seen:\n",
    "            continue\n",
    "        seen.add(term)\n",
    "        cleaned.append(f\"{term}:{m['score']}\" if with_scores else term)\n",
    "        if len(cleaned) >= topn:\n",
    "            break\n",
    "    return cleaned\n",
    "\n",
    "def pick_final_with_death_guard(text, combined, evidence):\n",
    "    cand = max(combined.items(), key=lambda x: x[1])[0]\n",
    "    if not ENFORCE_DEATH_CONFIRMATION or cand != \"Death\":\n",
    "        return cand, \"hybrid_fallback\"\n",
    "    strict = has_strict_death(text)\n",
    "    evid_ok = evidence.get(\"Death\", 0.0) >= DEATH_EVID_MIN\n",
    "    if strict and evid_ok:\n",
    "        return \"Death\", \"hybrid_fallback_confirmed_death\"\n",
    "    non_death = {k: v for k, v in combined.items() if k != \"Death\"}\n",
    "    alt = max(non_death.items(), key=lambda x: x[1])[0]\n",
    "    return alt, \"death_demoted_insufficient_evidence\"\n",
    "\n",
    "# ================== Your classify_row (unchanged) ==================\n",
    "def classify_row(text, predicted_probability):\n",
    "    text = text\n",
    "    prob = parse_prob_dict(predicted_probability)\n",
    "    prob = {lab: float(prob.get(lab, 0.0)) for lab in LABELS}\n",
    "\n",
    "    sorted_labs = sorted(LABELS, key=lambda l: prob[l], reverse=True)\n",
    "    top1, top2 = sorted_labs[0], sorted_labs[1]\n",
    "    p1, p2 = prob[top1], prob[top2]\n",
    "    margin = p1 - p2\n",
    "\n",
    "    high_conf_and_clear = (p1 >= HI_THRESH) and (margin > MARGIN_THRESH)\n",
    "    labels_to_extract = [top1] if high_conf_and_clear else LABELS\n",
    "\n",
    "    evidence = {lab: 0.0 for lab in LABELS}\n",
    "    matches  = {lab: []  for lab in LABELS}\n",
    "    for lab in labels_to_extract:\n",
    "        e, m = evidence_for_label(text, lab)\n",
    "        evidence[lab], matches[lab] = e, m\n",
    "\n",
    "    if high_conf_and_clear:\n",
    "        final_label = top1\n",
    "        final_score = p1\n",
    "        decision = \"model_confident\"\n",
    "    else:\n",
    "        alpha = ALPHA_LOW if margin <= MARGIN_THRESH else ALPHA_DEFAULT\n",
    "        combined = {lab: alpha*prob[lab] + (1-alpha)*evidence[lab] for lab in LABELS}\n",
    "        final_label, decision = pick_final_with_death_guard(text, combined, evidence)\n",
    "        final_score = round(combined[final_label], 3)\n",
    "\n",
    "    flat_matches = {f\"{lab.lower().replace(' ', '_')}_matches\": json.dumps(matches[lab], ensure_ascii=False) for lab in LABELS}\n",
    "\n",
    "    clean_cols = {}\n",
    "    for lab in LABELS:\n",
    "        terms_only = clean_keyword_list(matches.get(lab, []), topn=8, with_scores=False)\n",
    "        terms_with_scores = clean_keyword_list(matches.get(lab, []), topn=8, with_scores=True)\n",
    "        clean_cols[f\"{lab.lower().replace(' ', '_')}_keywords\"] = \"; \".join(terms_only)\n",
    "        clean_cols[f\"{lab.lower().replace(' ', '_')}_keywords_scored\"] = \"; \".join(terms_with_scores)\n",
    "\n",
    "    return {\n",
    "        \"final_label\": final_label,\n",
    "        \"final_score\": final_score,\n",
    "        \"decision\": decision,\n",
    "        \"p_model\": json.dumps({lab: round(prob[lab],3) for lab in LABELS}),\n",
    "        \"evidence\": json.dumps({lab: round(evidence[lab],3) for lab in LABELS}),\n",
    "        **flat_matches,\n",
    "        **clean_cols\n",
    "    }\n",
    "\n",
    "# ================== LLM probs using your method ==================\n",
    "def build_prompt(text: str):\n",
    "    instruction = \"Classify the type of adverse event as Death, Injury, or Device Malfunction.\"\n",
    "    return f\"\"\"{instruction}\n",
    "\n",
    "Event: {text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def llm_label_probs(report_text: str, tokenizer, model) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Your original method: generate 1st token, read its distribution,\n",
    "    take probability mass on the first token of each label.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(report_text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5,   # tiny, we only need the first step scores\n",
    "            do_sample=False,\n",
    "            use_cache=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "    # Decode full prediction (optional, not used downstream)\n",
    "    decoded = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    _prediction = decoded.split(\"Answer:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    transition_scores = outputs.scores\n",
    "    if not transition_scores:\n",
    "        return {lab: 0.0 for lab in LABELS}\n",
    "\n",
    "    step_logits = transition_scores[0]  # first step logits\n",
    "    step_probs = F.softmax(step_logits, dim=-1)\n",
    "    label_probs = {}\n",
    "    for lab in LABELS:\n",
    "        lab_tok = tokenizer(lab, add_special_tokens=False).input_ids\n",
    "        label_probs[lab] = step_probs[0, lab_tok[0]].item() if lab_tok else 0.0\n",
    "\n",
    "    # normalize\n",
    "    s = sum(label_probs.values()) or 1.0\n",
    "    return {k: v/s for k, v in label_probs.items()}\n",
    "\n",
    "# ================== Compose FOI answer with Ollama ==================\n",
    "def build_ollama_reply_prompt(report_text: str, analysis: Dict[str, Any], role: str, audience: str) -> str:\n",
    "    return f\"\"\"Role: {role}\n",
    "Audience: {audience}\n",
    "\n",
    "User provided an FOI narrative report (no database search). Analyze the text and provide a clear answer.\n",
    "\n",
    "FOI TEXT:\n",
    "\\\"\\\"\\\"{report_text.strip()}\\\"\\\"\\\"\n",
    "\n",
    "Structured findings from our internal classifier:\n",
    "- Final label: {analysis['final_label']} (score {analysis['final_score']})\n",
    "- Decision path: {analysis['decision']}\n",
    "- Model probabilities: {analysis['p_model']}\n",
    "- Keyword evidence: {analysis['evidence']}\n",
    "- Top evidence terms:\n",
    "  ‚Ä¢ Injury: {analysis.get('injury_keywords','')}\n",
    "  ‚Ä¢ Device Malfunction: {analysis.get('device_malfunction_keywords','')}\n",
    "  ‚Ä¢ Death: {analysis.get('death_keywords','')}\n",
    "\n",
    "Write a concise, clinician-friendly answer that:\n",
    "1) states the likely AE type and confidence,\n",
    "2) explains the key evidence in plain language,\n",
    "3) flags any missing info or ambiguities,\n",
    "4) avoids speculation beyond the provided text.\n",
    "\"\"\"\n",
    "\n",
    "# ================== Gradio UI (with FOI path) ==================\n",
    "# ---- optional: build these from your dataframe if available ----\n",
    "try:\n",
    "    ae_types = [\"Injury\", \"Device Malfunction\", \"Death\"]\n",
    "    years_list = sorted(df[\"year\"].dropna().astype(int).unique().tolist())\n",
    "    manufacturers_list = sorted(df[\"manufacturer\"].dropna().astype(str).unique().tolist())\n",
    "\n",
    "    def _safe_kw(col):\n",
    "        out = []\n",
    "        if col in df.columns:\n",
    "            for v in df[col].dropna().tolist():\n",
    "                if isinstance(v, list):\n",
    "                    out.extend(v)\n",
    "                else:\n",
    "                    try:\n",
    "                        import re as _re\n",
    "                        if isinstance(v, str) and v.strip().startswith((\"[\",\"{\")):\n",
    "                            vv = json.loads(v)\n",
    "                            if isinstance(vv, list): out.extend(vv)\n",
    "                        else:\n",
    "                            for tok in _re.split(r\"[;,\\|\\n]+\", str(v)):\n",
    "                                tok = tok.strip()\n",
    "                                if tok: out.append(tok)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        return out\n",
    "\n",
    "    keywords_list = sorted({k.strip().lower()\n",
    "                            for k in (_safe_kw(\"injury_keywords\")\n",
    "                                      + _safe_kw(\"malfunction_keywords\")\n",
    "                                      + _safe_kw(\"death_keywords\"))\n",
    "                            if k})\n",
    "except Exception:\n",
    "    ae_types = [\"Injury\", \"Device Malfunction\", \"Death\"]\n",
    "    years_list = []\n",
    "    manufacturers_list = []\n",
    "    keywords_list = []\n",
    "\n",
    "def build_gradio_chat():\n",
    "    def _build_filters(sel_labels: List[str], sel_years: List[int], sel_mfgs: List[str],\n",
    "                       report_key: str, sel_keywords: List[str]) -> Dict[str, Any]:\n",
    "        c: Dict[str, Any] = {}\n",
    "        if sel_mfgs:   c[\"manufacturer\"] = sel_mfgs\n",
    "        if sel_labels: c[\"label\"] = sel_labels\n",
    "        if sel_years:  c[\"years\"] = sel_years\n",
    "        if report_key and str(report_key).strip():\n",
    "            try:\n",
    "                ids = [int(x) for x in str(report_key).replace(\" \", \"\").split(\",\") if x]\n",
    "                c[\"report_keys\"] = ids if len(ids) > 1 else ids[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "        if sel_keywords: c[\"keywords\"] = [k.strip().lower() for k in sel_keywords]\n",
    "        return c\n",
    "\n",
    "    def chat_step(history, message, role, audience, preset, temperature, num_predict,\n",
    "                  sel_labels, sel_years, sel_mfgs, report_key, sel_keywords,\n",
    "                  analyze_toggle, report_box):\n",
    "        history = history or []\n",
    "        msg = (message or \"\").strip()\n",
    "        report_text = (report_box or \"\").strip()\n",
    "\n",
    "        # Show user message immediately (or a placeholder if FOI mode)\n",
    "        shown_user = msg if not analyze_toggle else (msg or \"[FOI analysis request]\")\n",
    "        history.append((shown_user, None))\n",
    "\n",
    "        try:\n",
    "            if analyze_toggle and report_text:\n",
    "                # -------- FOI path (NO QDRANT) --------\n",
    "                # 1) LLM probs\n",
    "                p_dict = llm_label_probs(report_text, tokenizer, model)  # dict[label]->prob\n",
    "                # 2) Hybrid classify\n",
    "                analysis = classify_row(report_text, p_dict)\n",
    "                # 3) Ask Ollama to write the final answer using the extracted info\n",
    "                foi_prompt = build_ollama_reply_prompt(report_text, analysis, role, audience)\n",
    "                reply = ask_question(\n",
    "                    foi_prompt,\n",
    "                    role=role,\n",
    "                    audience=audience,\n",
    "                    preset=preset,\n",
    "                    model=OLLAMA_MODEL,\n",
    "                    url=OLLAMA_URL,\n",
    "                    temperature=float(temperature),\n",
    "                    num_predict=int(num_predict),\n",
    "                )\n",
    "            else:\n",
    "                # -------- Normal RAG path (with optional filters) --------\n",
    "                filters = _build_filters(sel_labels, sel_years, sel_mfgs, report_key, sel_keywords)\n",
    "                reply = ask_question(\n",
    "                    msg,\n",
    "                    role=role,\n",
    "                    audience=audience,\n",
    "                    preset=preset,\n",
    "                    model=OLLAMA_MODEL,\n",
    "                    url=OLLAMA_URL,\n",
    "                    temperature=float(temperature),\n",
    "                    num_predict=int(num_predict),\n",
    "                    filters=filters,   # your existing behavior\n",
    "                )\n",
    "        except TypeError:\n",
    "            # Back-compat if ask_question doesn't accept filters\n",
    "            reply = ask_question(\n",
    "                msg if not (analyze_toggle and report_text) else foi_prompt,\n",
    "                role=role,\n",
    "                audience=audience,\n",
    "                preset=preset,\n",
    "                model=OLLAMA_MODEL,\n",
    "                url=OLLAMA_URL,\n",
    "                temperature=float(temperature),\n",
    "                num_predict=int(num_predict),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            reply = f\"‚ö†Ô∏è Error: {e}\"\n",
    "\n",
    "        history[-1] = (shown_user, reply)\n",
    "        return history, \"\", \"\"  # clear both inputs\n",
    "\n",
    "    with gr.Blocks(title=\"Qdrant + Ollama RAG Chatbot\") as demo:\n",
    "        gr.Markdown(\"## üßë‚Äç‚öïÔ∏è Medical Device RAG Chatbot\\nAsk natural questions; or paste an FOI report for one-off analysis (no DB search).\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(height=540)\n",
    "                user_box = gr.Textbox(\n",
    "                    placeholder='Examples: list manufacturers; count Injury in 2024; reports with \"vascular laceration\" in 2021.',\n",
    "                    lines=2,\n",
    "                    label=\"Your question\"\n",
    "                )\n",
    "                with gr.Accordion(\"Analyze FOI (no search)\", open=False):\n",
    "                    analyze_toggle = gr.Checkbox(label=\"Use FOI analysis (skip database search)\", value=False)\n",
    "                    report_box = gr.Textbox(\n",
    "                        label=\"Paste FOI report\",\n",
    "                        placeholder=\"Paste the full FOI narrative here‚Ä¶\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "                    clear_btn  = gr.Button(\"Clear\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                gr.Markdown(\"### Filters (optional, DB search mode)\")\n",
    "                sel_labels = gr.CheckboxGroup(choices=ae_types, label=\"AE type (final_label)\")\n",
    "                sel_years = gr.Dropdown(choices=years_list, multiselect=True, label=\"Year(s)\")\n",
    "                sel_mfgs = gr.Dropdown(choices=manufacturers_list, multiselect=True, label=\"Manufacturer(s)\")\n",
    "                report_key = gr.Textbox(label=\"Report key(s) (comma-separated)\")\n",
    "                sel_keywords = gr.Dropdown(choices=keywords_list, multiselect=True, label=\"Keywords\")\n",
    "\n",
    "                gr.Markdown(\"### Settings\")\n",
    "                role        = gr.Textbox(label=\"Role\", value=\"clinical safety analyst\")\n",
    "                audience    = gr.Textbox(label=\"Audience\", value=\"scientifically literate\")\n",
    "                preset      = gr.Dropdown(\n",
    "                    choices=[\"exec_summary\",\"technical\",\"detailed\",\"support_reply\"],\n",
    "                    value=\"exec_summary\",\n",
    "                    label=\"Style preset\"\n",
    "                )\n",
    "                temperature = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label=\"Temperature\")\n",
    "                num_predict = gr.Slider(128, 2048, value=700, step=32, label=\"Max tokens (num_predict)\")\n",
    "\n",
    "        # Wire events\n",
    "        submit_btn.click(\n",
    "            fn=chat_step,\n",
    "            inputs=[chatbot, user_box, role, audience, preset, temperature, num_predict,\n",
    "                    sel_labels, sel_years, sel_mfgs, report_key, sel_keywords,\n",
    "                    analyze_toggle, report_box],\n",
    "            outputs=[chatbot, user_box, report_box],\n",
    "        )\n",
    "        user_box.submit(\n",
    "            fn=chat_step,\n",
    "            inputs=[chatbot, user_box, role, audience, preset, temperature, num_predict,\n",
    "                    sel_labels, sel_years, sel_mfgs, report_key, sel_keywords,\n",
    "                    analyze_toggle, report_box],\n",
    "            outputs=[chatbot, user_box, report_box],\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            fn=lambda: ([], \"\", \"\"),\n",
    "            inputs=None,\n",
    "            outputs=[chatbot, user_box, report_box],\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "# --------- launch ---------\n",
    "demo = build_gradio_chat()\n",
    "# demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
